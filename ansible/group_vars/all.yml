---
# The options in this file can be overridden in 'globals.yml'

# The "temp" files that are created before merge need to stay persistent due
# to the fact that ansible will register a "change" if it has to create them
# again. Persistent files allow for idempotency
container_config_directory: "/var/lib/kolla-ceph/config_files"

# The directory to merge custom config files the kolla-ceph's config files
node_custom_config: "{{ CONFIG_DIR }}"

# The directory to store the config files on the destination node
node_config_directory: "/etc/kolla-ceph"

# The group which own node_config_directory, you can use a non-root
# user to deploy kolla-ceph
config_owner_user: "root"
config_owner_group: "root"


###################
# Kolla options
###################

# Valid options are [ COPY_ONCE, COPY_ALWAYS ]
config_strategy: "COPY_ALWAYS"

# Valid options are ['centos']
kolla_base_distro: "centos"
# Valid options are [ binary, source ]
kolla_install_type: "binary"

# By default, Kolla API services bind to the network address assigned
# to the api_interface.  Allow the bind address to be an override.
api_interface_address: "{{ hostvars[inventory_hostname]['ansible_' + api_interface]['ipv4']['address'] }}"

# This is used to get the ip corresponding to the storage_interface.
storage_interface_address: "{{ hostvars[inventory_hostname]['ansible_' + storage_interface]['ipv4']['address'] }}"

enable_fluentd: "yes"
enable_ceph_mds: "yes"
enable_ceph_rgw: "yes"
enable_ceph_dashboard: "yes"
enable_prometheus_ceph_mgr_exporter: "no"

fluentd_syslog_port: "5140"
####################
# Docker options
####################
docker_registry_email:
docker_registry:
docker_namespace: "kolla-ceph"
docker_registry_username:
docker_registry_password:
docker_runtime_directory: ""

# Retention settings for Docker logs
docker_log_max_file: 5
docker_log_max_size: 50m

# Valid options are [ no, on-failure, always, unless-stopped ]
docker_restart_policy: "unless-stopped"

# '0' means unlimited retries (applies only to 'on-failure' policy)
docker_restart_policy_retry: "10"

# Common options used throughout Docker
docker_common_options:
  auth_email: "{{ docker_registry_email }}"
  auth_password: "{{ docker_registry_password }}"
  auth_registry: "{{ docker_registry }}"
  auth_username: "{{ docker_registry_username }}"
  environment:
    KOLLA_CONFIG_STRATEGY: "{{ config_strategy }}"
  restart_policy: "{{ docker_restart_policy }}"
  restart_retries: "{{ docker_restart_policy_retry }}"

#######################
# Extra volumes options
#######################
# Extra volumes for Docker Containers
default_extra_volumes: []

####################
# Dimensions options
####################
# Dimension options for Docker Containers
default_container_dimensions: {}

###################
# Ceph options
###################

# The default tag used for docker image, not recommended
ceph_release: ""

ceph_logging_debug: "no"

ceph_cluster_fsid: ""
# Ceph can be setup with a caching to improve performance. To use the cache you
# must provide separate disks than those for the OSDs
ceph_enable_cache: "no"

# Ceph is not able to determine the size of a cache pool automatically,
# so the configuration on the absolute size is required here, otherwise the flush/evict will not work.
ceph_target_max_bytes: ""
ceph_target_max_objects: ""

# Valid options are [ forward, none, writeback ]
ceph_cache_mode: "writeback"

# Valid options are [ ext4, btrfs, xfs ]
ceph_osd_filesystem: "xfs"

# Set to 'yes-i-really-really-mean-it' to force wipe disks with existing partitions for OSDs. Only
# set if you understand the consequences!
ceph_osd_wipe_disk: ""

# These are /etc/fstab options. Comma separated, no spaces (see fstab(8))
ceph_regular_mount_options: "defaults,noatime"
ceph_iscsi_mount_options: "defaults,_netdev,noatime"
ceph_tmpfs_mount_options: "nodev,nosuid"

# A requirement for using the erasure-coded pools is you must setup a cache tier
# Valid options are [ erasure, replicated ]
ceph_pool_type: "replicated"

# When ceph is upgraded, whether to enable cluster status monitoring.
# When the value is yes, please confirm that the status of the ceph
# cluster is HEALTH_OK and then execute.
enable_upgrade_health_check: "no"

# Whether to restart the container when it is running. This parameter is only
# valid in deploy action and is higher than other restart conditions.
ceph_container_running_restart: "yes"

# Whether to restart the container when the ceph.conf file changes
ceph_conf_change_restart: "no"

# Maximum number and interval of ceph cluster check
ceph_health_check_retries: 15
ceph_health_check_delay: 20

# Kolla-ceph supports the device classes feature of ceph. Please configure the device
# class in the mutinode inventory first, then use it.
# If you use the device class(such as hdd and ssd), you can define the pool as follows:
# ceph_erasure_profile: "k=4 m=2 ruleset-failure-domain=host crush-device-class=ssd"
# ceph_rule: "default host hdd"
# ceph_cache_rule: "cache host ssd"

ceph_erasure_name: "erasure-profile"
ceph_erasure_profile: "k=2 m=1 ruleset-failure-domain=host"
ceph_rule_name: "disks"
ceph_rule: "default host"
ceph_cache_rule_name: "cache"
ceph_cache_rule: "cache host"

# Set the pgs and pgps for pool
# WARNING! These values are dependant on the size and shape of your cluster -
# the default values are not suitable for production use. Please refer to the
# Kolla Ceph documentation for more information.
ceph_pool_pg_num: 8
ceph_pool_pgp_num: 8

# Set the host type for ceph daemons
# Valid options are [ IP, HOSTNAME, FQDN, INVENTORY ]
# Note: For existing clusters, please don't modify this parameter. Otherwise,
# the existing mon will be invalidated, and the existing osd crush map will
# be changed.
ceph_mon_host_type: "IP"
ceph_mgr_host_type: "INVENTORY"
ceph_osd_host_type: "IP"
ceph_mds_host_type: "INVENTORY"

# Set the ceph daemon you want to install in a deployment.
# This parameter is valid for daemons that are already enabled. When the
# value of the component is no, the corresponding daemon will be skipped in
# the deployment. This allows the user to modify only some daemons instead of all.
ceph_install_daemons:
  - 'ceph-mon'
  - 'ceph-mgr'
  - 'ceph-osd'
  - 'ceph-rgw'
  - 'ceph-mds'

############
# ceph pools
############
ceph_pools:
  - pool_name: "rbd"
    pool_type: "replicated"
    pool_rule_name: "hdd-rep"
    pool_rule: "default host hdd"
    pool_pg_num: 32
    pool_pgp_num: 32
    pool_application: "rbd"
    create: "yes"
  - pool_name: "rbd-ec"
    pool_type: "erasure"
    pool_erasure_name: "hdd-ec"
    pool_erasure_profile: "k=2 m=1 crush-failure-domain=osd crush-device-class=hdd"
    pool_pg_num: 32
    pool_pgp_num: 32
    pool_application: "rbd"
    create: "yes"
